Individuals who are deaf (with difficulty hearing) and mute primarily communicate using Sign Language (SL) through hand and body gestures. SL has a unique vocabulary, meaning, and syntax, distinct from spoken or written languages. Currently, an estimated 138 to 300 types of SL are used worldwide. In India, despite having a deaf population of 7 million, there are only 250 certified SL interpreters. 
The development of this model will significantly improve communication for the deaf (hard of hearing) and mute populations. The SignSpeak project starts by recognizing hand movements and eventually produces text and speech corresponding to hand gestures.
This project aims to create a real-time SL translation system to address the communication gap between people who are deaf and those who can hear. Sign Language Recognition (SLR) involves recognizing hand gestures and translating them into text and then to speech. Hand gestures in SL have been classified into static and dynamic categories. 
A static gesture is depicted as a single image of a hand configuration and pose, while a dynamic gesture is represented by a series of images showing movement. Our project stands out by having the ability to translate hand gestures into various regional languages, making it more accessible and inclusive for a wider audience. 
